{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata_path = 'data/metadata.csv'\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# Load and preprocess images\n",
    "def load_and_preprocess_images(image_paths, target_size=(256, 256)):\n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        img = tf.image.decode_jpeg(tf.io.read_file(path))\n",
    "        img = tf.image.resize(img, target_size)\n",
    "        img = img / 255.0\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "image_paths = ['data/' + img_id + '.jpg' for img_id in metadata['isic_id']]\n",
    "images = load_and_preprocess_images(image_paths)\n",
    "\n",
    "# Prepare target masks\n",
    "mask_paths = ['data/masks/' + img_id + '_mask.jpg' for img_id in metadata['isic_id']]\n",
    "masks = load_and_preprocess_images(mask_paths)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_images, test_images, train_masks, test_masks = train_test_split(images, masks, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(input_shape, num_channels=3):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # Contracting Path (Encoder)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    # Expanding Path (Decoder)\n",
    "    up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    up6 = Conv2D(512, 2, activation='relu', padding='same')(up6)\n",
    "    merge6 = concatenate([drop4, up6], axis=3)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    up7 = Conv2D(256, 2, activation='relu', padding='same')(up7)\n",
    "    merge7 = concatenate([conv3, up7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    up8 = Conv2D(128, 2, activation='relu', padding='same')(up8)\n",
    "    merge8 = concatenate([conv2, up8], axis=3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = UpSampling2D(size=(2, 2))(conv8)\n",
    "    up9 = Conv2D(64, 2, activation='relu', padding='same')(up9)\n",
    "    merge9 = concatenate([conv1, up9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    # Build the model\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Create the U-Net model\n",
    "input_shape = (256, 256, 3)  # RGB images\n",
    "model = unet_model(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation with moderate techniques\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='reflect'\n",
    ")\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(patience=25, restore_best_weights=True)\n",
    "\n",
    "# Learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "    if epoch < 20:\n",
    "        return 0.001\n",
    "    elif epoch < 40:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.00001\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model (already compiled, you can remove this part)\n",
    "model = unet_model(input_shape=(256, 256, 3))\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained model weights\n",
    "model.load_weights('model_weights.h5') \n",
    "\n",
    "# Retrain with the complete dataset and fine-tuned hyperparameters\n",
    "batch_size = 16  # Experiment with batch size\n",
    "epochs = 50  # Adjust as needed\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(train_images, train_masks, batch_size=batch_size),\n",
    "    steps_per_epoch=len(train_images) // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(test_images, test_masks),  # Use a validation split to monitor performance\n",
    "    callbacks=[early_stopping, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear previous model and release memory\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Update learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "    initial_lr = 0.001\n",
    "    decay_rate = 0.5\n",
    "    decay_steps = 10\n",
    "    new_lr = initial_lr * (decay_rate ** (epoch // decay_steps))\n",
    "    return new_lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Update batch size and epochs\n",
    "batch_size = 18  # Experiment with different batch sizes\n",
    "epochs = 100    # Increase the number of epochs\n",
    "\n",
    "# Increase early stopping patience\n",
    "early_stopping = EarlyStopping(patience=50, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ModelCheckpoint callback\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'updated_model_weights.h5',  # Path to save the updated weights\n",
    "    monitor='val_loss',          # Monitor validation loss\n",
    "    save_best_only=True,         # Save only the best model\n",
    "    mode='min'                   # Mode can be 'min' or 'max' depending on the monitored metric\n",
    ")\n",
    "\n",
    "# Load pre-trained model weights\n",
    "model.load_weights('model_weights.h5') \n",
    "\n",
    "# Start retraining\n",
    "history = model.fit(\n",
    "    datagen.flow(train_images, train_masks, batch_size=batch_size),\n",
    "    steps_per_epoch=len(train_images) // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(test_images, test_masks),  \n",
    "    callbacks=[early_stopping, lr_scheduler, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(test_images, test_masks, batch_size=batch_size)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Predict masks on the test set\n",
    "test_predictions = model.predict(test_images)\n",
    "\n",
    "# Convert the predictions and labels to NumPy arrays\n",
    "predictions = np.array(test_predictions)\n",
    "labels = np.array(test_masks)\n",
    "\n",
    "# Plot the predictions against the actual labels\n",
    "plt.scatter(predictions, labels)\n",
    "plt.xlabel(\"Predictions\")\n",
    "plt.ylabel(\"Actual Labels\")\n",
    "plt.title(\"Predictions vs. Actual Labels\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
